{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76beb14a",
   "metadata": {},
   "source": [
    "# m_qubits_QVC_Breast_Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9115f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import svm\n",
    "\n",
    "import scipy\n",
    "from scipy.linalg import expm\n",
    "import scikitplot as skplt\n",
    "\n",
    "from h_partitioned import *\n",
    "#from W_unitary import *\n",
    "#from U_unitary import *\n",
    "from qiskit_algorithms.optimizers import COBYLA, ADAM, SPSA, SLSQP, POWELL, L_BFGS_B, TNC, AQGD\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6,4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7c0f5",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bc64761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_meshgrid(x1, x2, h=0.2):\n",
    "    \n",
    "    x1_min, x1_max = x1.min() - 1, x1.max() + 1\n",
    "    x2_min, x2_max = x2.min() - 1, x2.max() + 1\n",
    "    x1x1, x2x2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "    \n",
    "    return x1x1, x2x2\n",
    "\n",
    "\n",
    "def training_split(X_train, y_train, n_batches):\n",
    "    \n",
    "    if len(X_train)%n_batches == 0:\n",
    "        \n",
    "        X_batches = np.split(X_train, n_batches)\n",
    "        y_batches = np.split(y_train, n_batches)\n",
    "        \n",
    "    else:\n",
    "        print('Warning: the training set must be divided into equally sized batches')\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "\n",
    "def k_fold_split(X, y, ele_per_split, i):\n",
    "    \n",
    "    k_X_train = np.concatenate( (X[:ele_per_split*i, :], X[ele_per_split*(i+1):, :]) )\n",
    "    k_X_test = X[ele_per_split*i:ele_per_split*(i+1), :]\n",
    "    \n",
    "    k_y_train = np.concatenate( (y[:ele_per_split*i], y[ele_per_split*(i+1):]) )\n",
    "    k_y_test = y[ele_per_split*i:ele_per_split*(i+1)]\n",
    "    \n",
    "    return k_X_train, k_X_test, k_y_train, k_y_test\n",
    "\n",
    "\n",
    "def PCA1(X_train, X_test, y_train, y_test, n_dimensions):\n",
    "    \n",
    "    # Now the dataset's features will be standardized\n",
    "    # to fit a normal distribution.\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # To be able to use this data with the given\n",
    "    # number of qubits, the data must be broken down from\n",
    "    # 30 dimensions to `n` dimensions.\n",
    "    # This is done with Principal Component Analysis (PCA),\n",
    "    # which finds patterns while keeping variation.\n",
    "    pca = PCA(n_dimensions).fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    # The last step in the data processing is\n",
    "    # to scale the data to be between -1 and 1\n",
    "    samples = np.append(X_train, X_test, axis=0)\n",
    "    minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "    X_train = minmax_scale.transform(X_train)\n",
    "    X_test = minmax_scale.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def PCA2(X, y, n_dimensions):\n",
    "    \n",
    "    # Now the dataset's features will be standardized\n",
    "    # to fit a normal distribution.\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # To be able to use this data with the given\n",
    "    # number of qubits, the data must be broken down from\n",
    "    # 30 dimensions to `n` dimensions.\n",
    "    # This is done with Principal Component Analysis (PCA),\n",
    "    # which finds patterns while keeping variation.\n",
    "    pca = PCA(n_dimensions).fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "    # The last step in the data processing is\n",
    "    # to scale the data to be between -1 and 1\n",
    "    samples = X\n",
    "    minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "    X = minmax_scale.transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f12b2",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f70612d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "\n",
    "used_points = 40 ## Must be multiple of 70\n",
    "\n",
    "X = dataset.data[:used_points]\n",
    "y = dataset.target[:used_points]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "n_dimensions = 4\n",
    "X_train, X_test, y_train, y_test = PCA1(X_train, X_test, y_train, y_test, n_dimensions)\n",
    "\n",
    "n_batches = 2 \n",
    "X_batches, y_batches = training_split(X_train, y_train, n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76507aa",
   "metadata": {},
   "source": [
    "## Classical SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f5e969",
   "metadata": {},
   "source": [
    "### linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5274100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aglas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "linear_kernel = svm.LinearSVC()\n",
    "linear_kernel.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f5df596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = linear_kernel.score(X_train, y_train)\n",
    "accuracy_test = linear_kernel.score(X_test, y_test)\n",
    "\n",
    "print(accuracy_train)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345fe64",
   "metadata": {},
   "source": [
    "### gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a88a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = svm.SVC(gamma = 'scale')\n",
    "gaussian_kernel.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a9890eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = gaussian_kernel.score(X_train, y_train)\n",
    "accuracy_test = gaussian_kernel.score(X_test, y_test)\n",
    "\n",
    "print(accuracy_train)\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603996e3",
   "metadata": {},
   "source": [
    "## Quantum SVM (explicit approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbeba259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial parameters: [2.35330497 5.97351416 4.59925358 3.76148219 0.98029403 0.98014248\n",
      " 0.3649501  5.44234523 3.77691701 4.44895122 0.12933619 6.09412333\n",
      " 5.23039137 1.33416598 1.14243996 1.15236452 1.91161039 3.2971419\n",
      " 2.71399059 1.82984665 3.84438512 0.87646578 1.83559896 2.30191935]\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "seed = np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n = 4\n",
    "d = 2\n",
    "n_part = 2\n",
    "\n",
    "init_theta = 2*np.pi*np.random.random(n*d*3)\n",
    "print('Initial parameters: '+ str(init_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80494259",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7bb32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_loss(theta, data, labels):\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(0, len(labels)):\n",
    "        if labels[i] == 0:\n",
    "            tmp.append(-1)\n",
    "        else:\n",
    "            tmp.append(1)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(0, len(labels)):\n",
    "        predictions.append(h_partitioned(data[i], n, n_part, d, shots, theta))\n",
    "    \n",
    "    error = []\n",
    "    for i in range(0, len(predictions)):\n",
    "        parity = predictions[i] - tmp[i]\n",
    "        error.append(parity)\n",
    "\n",
    "    norm = np.linalg.norm(error)\n",
    "    \n",
    "    return norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a606297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs = n_batches\n",
    "shots = 1024\n",
    "theta = init_theta\n",
    "thetas = []\n",
    "predictions = []\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "for i in tqdm(range(0, epochs)):\n",
    "    \n",
    "    prediction = np.zeros(len(y_batches[i]))\n",
    "    data = X_batches[i]\n",
    "    labels = y_batches[i]\n",
    "    \n",
    "    for j in range(0, len(data)):\n",
    "        prediction[j] = h_partitioned(data[j], n, n_part, d, shots, theta)\n",
    "        \n",
    "        objective_function = lambda theta: MSE_loss(theta, data, labels)\n",
    "        optimizer = COBYLA(maxiter=100)\n",
    "        \n",
    "        theta_opt = optimizer.minimize(objective_function, theta).x\n",
    "    \n",
    "    h_subtest = np.zeros(len(y_test))\n",
    "    for j in range(0, len(y_test)):\n",
    "        h_subtest[j] = h_partitioned(X_test[j], n, n_part, d, shots, theta_opt)\n",
    "    \n",
    "    train_result = 1 - ( ((sum(np.abs(2*labels-1-prediction)))/2) / len(labels) ) \n",
    "    test_result = 1 -  ( ((sum(np.abs(2*y_test-1-h_subtest)))/2) / len(y_test) ) \n",
    "    training_accuracies.append(train_result)\n",
    "    testing_accuracies.append(test_result)\n",
    "    \n",
    "    thetas.append(theta_opt)\n",
    "    predictions.append(prediction)\n",
    "    theta = theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75413c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracies: '+ str(training_accuracies))\n",
    "print('Training mean: '+ str(np.mean(training_accuracies)))\n",
    "print('Testing accuracies: '+ str(testing_accuracies))\n",
    "print('Testing mean: '+ str(np.mean(testing_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d89ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(training_accuracies, color='blue', label='training')\n",
    "plt.plot(testing_accuracies, color='red', linestyle=':',label='testing')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracies')\n",
    "plt.legend(loc=0, frameon=False)\n",
    "plt.savefig('BC_learning_partitioned.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d41a45",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(0, len(predictions)):\n",
    "    for j in range(0, len(predictions[i])):\n",
    "        tmp.append(predictions[i][j])\n",
    "        \n",
    "accuracy_train = 1 - ( ((sum(np.abs(2*y_train-1-tmp)))/2) / len(y_train) )\n",
    "print('Training accuracy: '+ str(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab20d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_test = []\n",
    "for i in range(0, len(y_test)):\n",
    "    h_test.append(h_partitioned(X_test[i], n, n_part, d, shots, theta_opt))\n",
    "\n",
    "accuracy_test = 1 - ( ((sum(np.abs(2*y_test-1-h_test)))/2) / len(y_test) ) \n",
    "print('Testing accuracy: '+ str(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30868c6b",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e1e700",
   "metadata": {},
   "source": [
    "### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "assert len(X)%k == 0 \n",
    "ele_per_split = int(len(X)/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116bc3a8",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = init_theta\n",
    "k_thetas = []\n",
    "k_training_accuracies = []\n",
    "k_testing_accuracies = []\n",
    "\n",
    "for i in tqdm(range(k)):\n",
    "    \n",
    "    k_X_train, k_X_test, k_y_train, k_y_test = k_fold_split(X, y, ele_per_split, i)\n",
    "    k_X_train, k_X_test, k_y_train, k_y_test = PCA1(k_X_train, k_X_test, k_y_train, k_y_test, n_dimensions)\n",
    "\n",
    "    objective_function = lambda theta: MSE_loss(theta, k_X_train, k_y_train)\n",
    "    optimizer = COBYLA(maxiter=100)\n",
    "    \n",
    "    theta_opt = optimizer.minimize(objective_function, theta).x\n",
    "\n",
    "    k_thetas.append(theta_opt)\n",
    "\n",
    "    k_train_predictions = np.zeros(len(k_y_train))\n",
    "    for j in range(0, len(k_y_train)):\n",
    "        k_train_predictions[j] = h_partitioned(k_X_train[j], n, n_part, d, shots, theta_opt) \n",
    "\n",
    "    k_test_predictions = np.zeros(len(k_y_test))\n",
    "    for j in range(0, len(k_y_test)):\n",
    "        k_test_predictions[j] = h_partitioned(k_X_test[j], n, n_part, d, shots, theta_opt) \n",
    "        \n",
    "    k_train_result = 1 - ( ((sum(np.abs(2*k_y_train-1-k_train_predictions)))/2) / len(k_y_train) )\n",
    "    k_test_result = 1 - ( ((sum(np.abs(2*k_y_test-1-k_test_predictions)))/2) / len(k_y_test) )\n",
    "    k_training_accuracies.append(k_train_result)\n",
    "    k_testing_accuracies.append(k_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracies: '+ str(k_training_accuracies))\n",
    "print('Training mean: '+ str(np.mean(k_training_accuracies)))\n",
    "print('Testing accuracies: '+ str(k_testing_accuracies))\n",
    "print('Testing mean: '+ str(np.mean(k_testing_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92dcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_training_accuracies, color='blue', label='training')\n",
    "plt.plot(k_testing_accuracies, color='red', linestyle=':',label='testing')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracies')\n",
    "plt.legend(loc=0, frameon=False)\n",
    "plt.savefig('BC_cv_partitioned.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbb4064",
   "metadata": {},
   "source": [
    "### valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_testing_accuracy = max(k_testing_accuracies)\n",
    "index = k_testing_accuracies.index(max_testing_accuracy)\n",
    "k_theta_opt = k_thetas[index]\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "X_val = dataset.data[:used_points]\n",
    "y_val = dataset.target[:used_points]\n",
    "\n",
    "X_val, y_val = PCA2(X_val, y_val, n_dimensions)\n",
    "\n",
    "h_val = np.zeros(len(y_val))\n",
    "for i in range(0, len(y_val)):\n",
    "    h_val[i] = h_partitioned(X_val[i], n, n_part, d, shots, k_theta_opt) \n",
    "\n",
    "validation_accuracy = 1 - ( ((sum(np.abs(2*y_val-1-h_val)))/2) / len(y_val) )\n",
    "\n",
    "print(\"Optimal parameters: \"+ str(k_theta_opt))\n",
    "print(\"Validation accuracy: \"+ str(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(0, len(h_val)):\n",
    "    if h_val[i] == 1:\n",
    "        tmp.append(0)\n",
    "    else:\n",
    "        tmp.append(1)\n",
    "        \n",
    "skplt.metrics.plot_confusion_matrix(y_val, tmp, normalize=True, title = 'Breast Cancer (after cross-validation)');\n",
    "plt.savefig('BC_cf_partitioned.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a94f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
